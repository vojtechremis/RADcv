{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vojtechremis/RADcv/blob/main/code/01RAD_Ex08_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS4YwqHLLi6P"
      },
      "source": [
        "# Linear Regression: Diagnostics and Influence Measures\n",
        "\n",
        "In this exercise, we recap:\n",
        "* **Data Generation, Regression Modeling, and Visualization**: Scatterplots for data exploration and regression diagnostic plots.\n",
        "\n",
        "and we explore:\n",
        "* **Diagnostics**: Examining residuals, leverage, and influence measures to evaluate model assumptions and detect outliers or influential observations.\n",
        "\n",
        "**The goal** is to understand how linear regression assumptions can be validated and how to identify problematic data points.\n",
        "\n",
        "---\n",
        "### Assumption snapshot (Gauss-Markov)\n",
        "- Linearity: the conditional mean of `Y` is a straight combination of predictors.\n",
        "- Independence: errors do not gossip with each other.\n",
        "- Homoscedasticity: constant variance of errors (no patterns).\n",
        "- No extreme multicollinearity: predictors are not linear combination each other.\n",
        "- Normal errors: useful for inference, less critical for point estimates.\n",
        "\n",
        "Gauss-Markov says that under the first four bullets, OLS is the Best Linear Unbiased Estimator (BLUE).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr8RRFaEi5DF"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from statsmodels.graphics.regressionplots import plot_ccpr_grid, plot_partregress_grid\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "from statsmodels.stats.outliers_influence import OLSInfluence\n",
        "from pathlib import Path\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZ6J4xeCLruH"
      },
      "outputs": [],
      "source": [
        "# Set a random seed for reproducibility\n",
        "np.random.seed(69)\n",
        "\n",
        "\n",
        "# Generate data for regression\n",
        "# Sample size and predictors\n",
        "n = 100\n",
        "p = 4\n",
        "\n",
        "# Error term\n",
        "e = np.random.normal(0, 4, n)\n",
        "\n",
        "# Coefficients\n",
        "beta0 = np.array([5, 3, 2, -5]).reshape(4, 1)\n",
        "\n",
        "# Predictors\n",
        "X0 = np.ones(n)\n",
        "X1 = np.random.normal(20, 3, n)\n",
        "X2 = 10 + np.random.exponential(1 / 0.1, n)\n",
        "X3 = 5 + np.random.binomial(15, 0.2, n)\n",
        "\n",
        "# Response variable Y\n",
        "Y = np.dot(np.column_stack((X0, X1, X2, X3)), beta0).flatten() + e\n",
        "\n",
        "# Create DataFrame\n",
        "data0 = pd.DataFrame({'X0': X0, 'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
        "\n",
        "# Selecting the variables\n",
        "X = data0[['X1', 'X2', 'X3']]\n",
        "Y = data0['Y']\n",
        "\n",
        "# Display basic information about the data\n",
        "print(data0.head())\n",
        "print(data0.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoN5OK3ilwmh"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Quick data health check and correlation map\n",
        "print(\"Missing values per column:\", data0.isna().sum().to_dict())\n",
        "\n",
        "corr = data0[['X1', 'X2', 'X3', 'Y']].corr()\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
        "plt.title('Correlation heatmap (predictors vs Y)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4errL1-sL3ez"
      },
      "source": [
        "## Scatterplots for Data Exploration\n",
        "\n",
        "Scatterplots help visualize relationships between predictors and the response variable (`Y`). These plots provide an initial understanding of the data and potential linear relationships.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu2lpBT1lwo8"
      },
      "outputs": [],
      "source": [
        "# Pairplot for exploratory data visualization\n",
        "sns.pairplot(data0, vars=['X1', 'X2', 'X3', 'Y'], diag_kind='kde')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dsgFqZ0IlwrP"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Pairwise fits to see marginal linearity\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for ax, col in zip(axes, ['X1', 'X2', 'X3']):\n",
        "    sns.regplot(x=data0[col], y=data0['Y'], ax=ax, scatter_kws={'alpha': 0.7}, line_kws={'color': 'red'})\n",
        "    ax.set_title(f\"{col} vs Y (corr={data0[col].corr(data0['Y']):.2f})\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9H5MNKVMNAY"
      },
      "source": [
        "## Fitting a Linear Regression Model\n",
        "\n",
        "The dataset includes three predictors (`X1`, `X2`, `X3`) and one response variable (`Y`). We'll fit an ordinary least squares (OLS) regression model to examine the relationship between the predictors and the response.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tjG51PHnG-A"
      },
      "outputs": [],
      "source": [
        "# Fit the regression model\n",
        "model = smf.ols('Y ~ X1 + X2 + X3', data=data0).fit()\n",
        "\n",
        "#X = sm.add_constant(X)  # Add an intercept term to the predictor variables\n",
        "#model = sm.OLS(Y, X).fit()\n",
        "\n",
        "# Displaying the summary of the model\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OohwzqGHo-8r"
      },
      "outputs": [],
      "source": [
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "X_vif = sm.add_constant(data0[['X1', 'X2', 'X3']])\n",
        "vif_df = pd.DataFrame({\n",
        "    'Feature': X_vif.columns,\n",
        "    'VIF': [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n",
        "})\n",
        "\n",
        "print('Variance Inflation Factors (rule of thumb: < 5 is usually fine):')\n",
        "print(vif_df)\n",
        "\n",
        "rmse = np.sqrt(model.mse_resid)\n",
        "print(f\"Training RMSE: {rmse:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vIQXR5cMhMF"
      },
      "source": [
        "## Diagnostic Plots\n",
        "\n",
        "Regression diagnostic plots provide insights into:\n",
        "- **Residual Behavior**: Check for non-linearity, heteroscedasticity, and outliers.\n",
        "- **Normality**: Evaluate the distribution of residuals.\n",
        "- **Influence and Leverage**: Identify observations that disproportionately affect the model.\n",
        "\n",
        "The key plots include:\n",
        "1. Fitted Values vs. Residuals\n",
        "2. Scale-Location Plot (Spread-Location plot)\n",
        "3. Normal Q-Q Plot\n",
        "4. Component-Residual (Partial Residual) Plots\n",
        "5. Added Variable (Partial Regression) Plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLQjo7eUPB3M"
      },
      "outputs": [],
      "source": [
        "def plot_regression_diagnostics(model):\n",
        "    \"\"\"\n",
        "    Generate diagnostic plots for a regression model with spline smoothing.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the diagnostic plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(15, 8))\n",
        "    standardized_residuals = model.get_influence().resid_studentized_internal\n",
        "    fitted_values = model.fittedvalues\n",
        "    scale_residuals = np.sqrt(np.abs(standardized_residuals))\n",
        "\n",
        "    # Plot of Fitted Values vs Residuals with LOESS smoothing\n",
        "    plt.subplot(2, 3, 1)\n",
        "    sns.regplot(\n",
        "        x=fitted_values,\n",
        "        y=model.resid,\n",
        "        lowess=True,\n",
        "        scatter_kws={'alpha': 0.7},\n",
        "        line_kws={'color': 'red'}\n",
        "    )\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel('Residuals')\n",
        "    plt.title('Fitted Values vs Residuals')\n",
        "\n",
        "\n",
        "    # Scale-Location plot with spline smoothing\n",
        "    plt.subplot(2, 3, 2)\n",
        "    sns.regplot(\n",
        "        x=model.fittedvalues,\n",
        "        y=np.sqrt(np.abs(standardized_residuals)),\n",
        "        lowess=True,\n",
        "        scatter_kws={'alpha': 0.7},\n",
        "        line_kws={'color': 'red'}\n",
        "    )\n",
        "    plt.axhline(0, color='red', linestyle='--')\n",
        "    plt.xlabel('Fitted Values')\n",
        "    plt.ylabel(r'$\\sqrt{|Standardized\\ Residuals|}$')  # LaTeX format\n",
        "    plt.title('Scale-Location')\n",
        "\n",
        "\n",
        "    # Normal Q-Q plot\n",
        "    plt.subplot(2, 3, 3)\n",
        "    sm.qqplot(model.resid, line='s', ax=plt.gca())\n",
        "    plt.title('Normal Q-Q')\n",
        "\n",
        "    # Response vs Residuals for each regressor\n",
        "    for i, col in enumerate(model.model.exog_names[1:], 2):\n",
        "        plt.subplot(2, 3, 2+i)\n",
        "        plt.scatter(model.model.exog[:, i - 1], model.resid, alpha=0.7)\n",
        "        plt.axhline(0, color='red', linestyle='--')\n",
        "        plt.xlabel(col)\n",
        "        plt.ylabel('Residuals')\n",
        "        plt.title(f'Response vs Residuals: {col}')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5BTGVkdTIz2"
      },
      "outputs": [],
      "source": [
        "fig = plot_regression_diagnostics(model)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeuRu-PlUcnA"
      },
      "source": [
        "## Component-Residual (Partial Residual) Plots\n",
        "\n",
        "Component-Residual Plots (Partial Residual Plots) are a useful diagnostic tool in regression analysis. They help to visualize the relationship between a predictor and the response variable while accounting for the effect of other predictors in the model.\n",
        "\n",
        "- Helps determine if a transformation of a predictor is necessary.\n",
        "- A **linear pattern** in the plot suggests that the relationship between the predictor and response is well-modeled.\n",
        "- Deviations from linearity (e.g., curvature) may indicate that the predictor's relationship with the response is non-linear.\n",
        "- **Outliers or influential points** may appear as points far away from the general pattern.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jjzv6P3ogXu"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.regressionplots import plot_ccpr_grid\n",
        "\n",
        "def plot_component_residuals(model):\n",
        "    \"\"\"\n",
        "    Generate Component-Residual Plots (Partial Residual Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Component-Residual Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_ccpr_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3SvesRxeUaCk"
      },
      "outputs": [],
      "source": [
        "fig1 = plot_component_residuals(model)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk4l3uDSKTRf"
      },
      "source": [
        "## Added Variable Plots\n",
        "Added Variable Plots are a useful diagnostic tool in regression. They help to visualize the contribution of each predictor variable to the response variable after accounting for other predictors.\n",
        "\n",
        "### Key Points:\n",
        "- **Purpose**: Show the partial relationship between a predictor and the response.\n",
        "- **Usage**: Identify whether a variable has a significant relationship with the response after adjusting for others.\n",
        "- **Interpretation**:\n",
        "  - A strong linear pattern indicates a significant relationship.\n",
        "  - Outliers or curvature may indicate a poor model fit or influential points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MYaaSX_ogZ4"
      },
      "outputs": [],
      "source": [
        "from statsmodels.graphics.regressionplots import plot_partregress_grid\n",
        "\n",
        "def plot_added_variable(model):\n",
        "    \"\"\"\n",
        "    Generate Added Variable Plots (Partial Regression Plots) for a regression model.\n",
        "\n",
        "    :param model: The fitted regression model object from statsmodels.\n",
        "    :return: A matplotlib figure object containing the Added Variable Plots.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    plot_partregress_grid(model, fig=fig)\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQrD5rXAoeW3"
      },
      "outputs": [],
      "source": [
        "fig2 = plot_added_variable(model)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQwPxep1ssv9"
      },
      "source": [
        "### Types of Residuals in Linear Regression (Recap from Ex4)\n",
        "\n",
        "$$\n",
        "Y_i = X_i \\beta + e_i, \\ \\text{where} \\ e_i \\sim N(0, \\sigma^2)\n",
        "$$\n",
        "\n",
        "Residuals measure the difference between observed and predicted values.\n",
        "\n",
        "#### 1. Raw Residuals\n",
        "\n",
        "The raw residuals are simply the differences between each observed value $ Y_i $ and its corresponding predicted value $\\hat{Y}_i $:\n",
        "$$\n",
        "\\hat{e}_i = Y_i - \\hat{Y}_i\n",
        "$$\n",
        "\n",
        "#### 2. Internally Studentized Residuals (unknown sigma)\n",
        "\n",
        "Internally studentized residuals adjust each residual to account for the leverage $ h_{ii} $ of each observation.\n",
        "\n",
        "$$\n",
        "\\hat{r_i} = \\frac{\\hat{e}_i}{s \\sqrt{1 - h_{ii}}}\n",
        "$$\n",
        "\n",
        "and $s^2 = \\hat{\\sigma}^2 = \\frac{1}{n - p}\\sum_{j=1}^n \\hat{e}_j^2 $ is the variance estimate from OLS, using all $n$ observations.\n",
        "\n",
        "\n",
        "Studentized Residuals better reflects the influence of each observation on the fit by normalizing based on individual variances. Internally studentized residuals do not fully assess an observation's influence if removed from the model.\n",
        "\n",
        "#### 3. Externally Studentized Residuals\n",
        "\n",
        "Externally Studentized Residuals $\\hat{r}_{(-i)}$\n",
        " - taking the PRESS residuals, or leave-one-out residuals (the residuals when each observation is left out of the model fit) and dividing by a scaled estimate of their standard deviation.\n",
        "$$\n",
        "\\hat{r}_{(-i)} =  \\frac{\\hat{e}_i}{s_{(-i)} \\sqrt{1 - h_{ii}}}\n",
        "$$\n",
        "where\n",
        "$$\n",
        "s_{(-i)} = \\frac{1}{n-p-1}SSS_{-i}\n",
        "$$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3x2bQYmcV3M"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Extract model details\n",
        "residuals = np.asarray(model.resid)  # Classical residuals\n",
        "h_ii = np.asarray(model.get_influence().hat_matrix_diag)  # Leverage values (h_ii)\n",
        "n = int(model.nobs)  # Number of observations\n",
        "p = int(model.df_model) + 1  # Number of parameters including intercept\n",
        "df_resid = int(model.df_resid)  # Residual degrees of freedom\n",
        "mse = model.mse_resid  # Mean squared error (s^2)\n",
        "\n",
        "# 2. Internal Studentized Residuals (matches resid_studentized in statsmodels)\n",
        "s_squared = np.sum(residuals**2) / df_resid  # OLS variance estimate\n",
        "studentized_residuals_internal = residuals / np.sqrt(s_squared * (1 - h_ii))\n",
        "\n",
        "# 3. External Studentized Residuals (matches resid_studentized_external in statsmodels)\n",
        "studentized_residuals_external = np.zeros_like(residuals, dtype=float)\n",
        "for i, (e_i, h_ii_i) in enumerate(zip(residuals, h_ii)):\n",
        "    # Leave-one-out standard deviation (s_{(-i)})\n",
        "    sse_minus_i = np.sum(residuals**2) - (e_i**2) / (1 - h_ii_i)\n",
        "    s_minus_i = np.sqrt(sse_minus_i / (df_resid - 1))\n",
        "\n",
        "    # Externally studentized residual\n",
        "    studentized_residuals_external[i] = e_i / (s_minus_i * np.sqrt(1 - h_ii_i))\n",
        "\n",
        "# Residuals from statsmodels for comparison\n",
        "model_studentized_residuals_internal = model.get_influence().resid_studentized  # Internal\n",
        "model_studentized_residuals_external = model.get_influence().resid_studentized_external  # External\n",
        "\n",
        "# Create a DataFrame for comparison\n",
        "residuals_df = pd.DataFrame({\n",
        "    'Classical Residuals (StatsModels)': residuals,\n",
        "    'Studentized Residuals (Internal - Hand)': studentized_residuals_internal,\n",
        "    'Studentized Residuals (External - Hand)': studentized_residuals_external,\n",
        "    'Studentized Residuals (Internal - StatsModels)': model_studentized_residuals_internal,\n",
        "    'Studentized Residuals (External - StatsModels)': model_studentized_residuals_external\n",
        "})\n",
        "\n",
        "# Display the first few rows\n",
        "residuals_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2y3o6gOtwm6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23HSszBstyFb"
      },
      "source": [
        "## Influence Measures in Linear Regression\n",
        "\n",
        "In linear regression, influence measures are used to identify observations that have a disproportionate impact on the model. These measures help in diagnosing the model's robustness and identifying outliers or influential points. Below are key influence measures commonly used:\n",
        "\n",
        "### 1. DFBETAS\n",
        "DFBETAS measures the difference in each coefficient estimate when an observation is omitted.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFBETAS_{ij} = \\frac{\\hat{\\beta}_j - \\hat{\\beta}_{j(i)}}{\\sqrt{\\hat{\\sigma}^2_{(i)} (X^T X)^{-1}_{jj}}}\n",
        "$$\n",
        "\n",
        "where $ \\hat{\\beta}_j $ is the estimated coefficient, $ \\hat{\\beta}_{j(i)} $ is the estimated coefficient with the $i$-th observation omitted, and $(X^T X)^{-1}_{jj} $ is the $j$-th diagonal element of the inverse of $X^T X $.\n",
        "\n",
        "### 2. DFFITS\n",
        "DFFITS is an influence statistic that measures the effect of deleting a single observation.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{i(i)}}{\\hat{\\sigma}_{(i)} \\sqrt{h_{ii}}}\n",
        "$$\n",
        "where $ \\hat{y}_i $ is the predicted value with all observations, $ \\hat{y}_{i(i)}$ is the predicted value with the $i$-th observation omitted, and $h_{ii}$ is the leverage of the $i$-th observation.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "Leverage values measure the influence of each observation on its own fitted value. High leverage points can significantly alter the position of the regression line.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "h_{ii} = X_i (X^T X)^{-1} X_i^T\n",
        "$$\n",
        "where $X_i$ is the $i$-th row of the matrix of predictors  $X$.\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "Covariance ratios compare the determinants of the covariance matrices with and without each observation. They help identify observations that influence the variance of the parameter estimates.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "CR_i = \\frac{\\det(\\hat{\\Sigma}_{(i)})}{\\det(\\hat{\\Sigma})}\n",
        "$$\n",
        "where $\\hat{\\Sigma}_{(i)}$ is the covariance matrix with the $i$-th observation omitted and $ \\hat{\\Sigma} $ is the covariance matrix with all observations.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "Cook's distance measures the effect of deleting a single observation on the entire regression model. It is a commonly used metric to identify influential observations.\n",
        "\n",
        "**Equation:**\n",
        "$$\n",
        "D_i = \\frac{\\sum_{j=1}^n (\\hat{y}_j - \\hat{y}_{j(i)})^2}{p \\hat{\\sigma}^2}\n",
        "$$\n",
        "where $ \\hat{y}_j $ is the predicted value for the $ j $-th observation, $ \\hat{y}_{j(i)} $ is the predicted value with the $i$-th observation omitted, $p$ is the number of predictors, and $ \\hat{\\sigma}^2 $ is the estimated variance of the residuals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DporrqiPuZY3"
      },
      "source": [
        "##\n",
        "\n",
        "### 1. DFBETAS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFBETAS for any coefficient exceeds $ \\frac{2}{\\sqrt{n}} $, where $ n $ is the number of observations.\n",
        "\n",
        "### 2. DFFITS\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if the absolute value of DFFITS is larger than $ 2 \\sqrt{\\frac{p+1}{n}} $, where $p$ is the number of predictors and $n$ is the number of observations.\n",
        "\n",
        "### 3. Leverage Values (h values)\n",
        "\n",
        "**Rule of Thumb:** An observation is considered to have high leverage if its leverage value exceeds $ \\frac{2(p+1)}{n} $, where $p$ is the number of predictors and $n$ is the number of observations.\n",
        "\n",
        "### 4. Covariance Ratios\n",
        "\n",
        "**Rule of Thumb:** There is no widely accepted rule of thumb for covariance ratios, but observations with values far from 1 (either much larger or smaller) are generally considered influential.\n",
        "\n",
        "### 5. Cook's Distances\n",
        "\n",
        "**Rule of Thumb:** An observation is considered influential if its Cook's distance $D_i > \\frac{4}{n} $, where $n$ is the number of observations.\n",
        "\n",
        "**Heuristics are starting points**: if several points hover near a cutoff, go deeper with context (use domain knowledge and raw plots).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y7JI4lfrrA8H"
      },
      "outputs": [],
      "source": [
        "# Old version of influence measures data frame\n",
        "def create_influence_dataframe(model):\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extracting the influence measures\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    covariance_ratios = influence.cov_ratio\n",
        "    cooks_distances = influence.cooks_distance[0]\n",
        "\n",
        "    # Creating the DataFrame\n",
        "    influence_df = pd.DataFrame({\n",
        "        'DFFITS': dffits,\n",
        "        'Leverage': leverage,\n",
        "        'Covariance Ratio': covariance_ratios,\n",
        "        'Cook\\'s Distance': cooks_distances\n",
        "    })\n",
        "\n",
        "    # Adding DFBETAS columns for each predictor\n",
        "    for i in range(dfbetas.shape[1]):\n",
        "        influence_df[f'DFBETA_{i}'] = dfbetas[:, i]\n",
        "\n",
        "    return influence_df\n",
        "\n",
        "influence_df = create_influence_dataframe(model)\n",
        "influence_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JS8y6EwdbhP"
      },
      "outputs": [],
      "source": [
        "def summarize_influence_measures(model):\n",
        "    \"\"\"\n",
        "    Summarize influence measures and flag observations as potential outliers.\n",
        "\n",
        "    :param model: Fitted regression model object from statsmodels.\n",
        "    :return: DataFrame summarizing influence measures and flagged outliers.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extract measures\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    cooks_distance = influence.cooks_distance[0]\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    cov_ratios = influence.cov_ratio\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n = int(model.nobs)\n",
        "    p = int(model.df_model)\n",
        "\n",
        "    # Rule of Thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Summarize outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        'High Cook\\'s Distance': cooks_distance > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(dfbetas.shape[1]):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        'Cook\\'s Distance': cooks_distance,\n",
        "        'DFFITS': dffits,\n",
        "        'Covariance Ratio': cov_ratios\n",
        "    })\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "summary = summarize_influence_measures(model)\n",
        "summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmAtUv42eYJX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def summarize_influence_measures_with_data(model, data):\n",
        "    \"\"\"\n",
        "    Summarize influence measures, flag outliers, and include original data columns.\n",
        "\n",
        "    :param model: Fitted regression model object from statsmodels.\n",
        "    :param data: DataFrame used to fit the regression model.\n",
        "    :return: DataFrame summarizing influence measures, flagged outliers, and original data.\n",
        "    \"\"\"\n",
        "    influence = model.get_influence()\n",
        "\n",
        "    # Extract measures\n",
        "    leverage = influence.hat_matrix_diag\n",
        "    cooks_distance = influence.cooks_distance[0]\n",
        "    dffits = influence.dffits[0]\n",
        "    dfbetas = influence.dfbetas\n",
        "    cov_ratios = influence.cov_ratio\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n = int(model.nobs)\n",
        "    p = int(model.df_model)\n",
        "\n",
        "    # Rule of Thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Summarize outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        \"High Cook's Distance\": cooks_distance > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(dfbetas.shape[1]):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        \"Cook's Distance\": cooks_distance,\n",
        "        'DFFITS': dffits,\n",
        "        'Covariance Ratio': cov_ratios\n",
        "    })\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    # Combine summary DataFrame with original data\n",
        "    summary_with_data = pd.concat([data.reset_index(drop=True), summary_df], axis=1)\n",
        "\n",
        "    #  Select rows where any flag is True\n",
        "    flagged_observations = summary_with_data.loc[summary_with_data.iloc[:, len(data.columns) + 4:].any(axis=1)]\n",
        "    return summary_with_data, flagged_observations\n",
        "\n",
        "# Example usage:\n",
        "summary_with_data, flagged_observations = summarize_influence_measures_with_data(model, data0)\n",
        "summary_with_data.head(), flagged_observations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FjlWKiEikWI"
      },
      "outputs": [],
      "source": [
        "def manual_leverage(model):\n",
        "    X = model.model.exog  # Extract design matrix\n",
        "    H = X @ np.linalg.inv(X.T @ X) @ X.T\n",
        "    return np.diag(H)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFUcmF1likYt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def manual_dfbetas(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    betas = np.asarray(model.params)  # Coefficients\n",
        "    n, p = X.shape\n",
        "    dfbetas = np.zeros((n, p))\n",
        "    xtx_inv = np.linalg.inv(X.T @ X)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute betas excluding observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "\n",
        "        # Leave-one-out sigma^2 for scaling\n",
        "        resid_minus_i = y_exclude_i - X_exclude_i @ betas_exclude_i\n",
        "        sigma_sq_minus_i = np.sum(resid_minus_i ** 2) / (n - p - 1)\n",
        "\n",
        "        # Compute DFBETAS for each predictor\n",
        "        for j in range(p):\n",
        "            dfbeta_raw = betas[j] - betas_exclude_i[j]\n",
        "            dfbetas[i, j] = dfbeta_raw / (np.sqrt(sigma_sq_minus_i) * np.sqrt(xtx_inv[j, j]))\n",
        "\n",
        "    return dfbetas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c44lgjQtikdj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def manual_dffits_basic(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    sigma = np.sqrt(model.mse_resid)  # Residual standard deviation\n",
        "    n, p = X.shape\n",
        "    dffits = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute predicted y for observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new_i = X[i] @ betas_exclude_i\n",
        "\n",
        "        # Compute DFFITS\n",
        "        h_ii = X[i] @ np.linalg.inv(X.T @ X) @ X[i].T  # Leverage for observation i\n",
        "        dffits[i] = (y_hat[i] - y_hat_new_i) / (sigma * np.sqrt(h_ii))\n",
        "\n",
        "    return dffits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAyvOZFvoFQI"
      },
      "outputs": [],
      "source": [
        "\n",
        "def manual_dffits_press(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    residuals = np.asarray(model.resid)  # Residuals\n",
        "    n, p = X.shape\n",
        "    dffits = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute betas and predicted y for observation i\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new_i = X[i] @ betas_exclude_i\n",
        "\n",
        "        # Compute leverage for observation i\n",
        "        h_ii = X[i] @ np.linalg.inv(X.T @ X) @ X[i].T\n",
        "\n",
        "        # Compute sigma^2_{(-i)} using leave-one-out residuals\n",
        "        resid_minus_i = y_exclude_i - X_exclude_i @ betas_exclude_i\n",
        "        sigma_sq_minus_i = np.sum(resid_minus_i ** 2) / (n - p - 1)\n",
        "        sigma_minus_i = np.sqrt(sigma_sq_minus_i)\n",
        "\n",
        "        # Compute DFFITS using sigma_{(-i)} and leverage\n",
        "        dffits[i] = (y_hat[i] - y_hat_new_i) / (sigma_minus_i * np.sqrt(h_ii))\n",
        "\n",
        "    return dffits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqyZe9UFipVp"
      },
      "outputs": [],
      "source": [
        "def manual_cooks_distances(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    y = model.model.endog  # Response variable\n",
        "    y_hat = model.fittedvalues  # Fitted values\n",
        "    sigma = np.sqrt(model.mse_resid)  # Residual standard deviation\n",
        "    n, p = X.shape\n",
        "    cooks_d = np.zeros(n)\n",
        "\n",
        "    for i in range(n):\n",
        "        # Leave-one-out X and y\n",
        "        X_exclude_i = np.delete(X, i, axis=0)\n",
        "        y_exclude_i = np.delete(y, i)\n",
        "\n",
        "        # Recompute predicted y for all observations\n",
        "        betas_exclude_i = np.linalg.inv(X_exclude_i.T @ X_exclude_i) @ (X_exclude_i.T @ y_exclude_i)\n",
        "        y_hat_new = X @ betas_exclude_i\n",
        "\n",
        "        # Compute Cook's Distance\n",
        "        cooks_d[i] = np.sum((y_hat - y_hat_new) ** 2) / (p * sigma**2)\n",
        "\n",
        "    return cooks_d\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaU-TYGgipav"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compute manual influence measures\n",
        "leverage = manual_leverage(model)\n",
        "dfbetas = manual_dfbetas(model)\n",
        "dffits_basic = manual_dffits_basic(model)\n",
        "dffits_press = manual_dffits_press(model)\n",
        "cooks_distances = manual_cooks_distances(model)\n",
        "\n",
        "# Compare with statsmodels\n",
        "influence = model.get_influence()\n",
        "statsmodels_leverage = influence.hat_matrix_diag\n",
        "statsmodels_dfbetas = influence.dfbetas\n",
        "statsmodels_dffits = influence.dffits[0]\n",
        "statsmodels_cooks = influence.cooks_distance[0]\n",
        "\n",
        "# Print comparisons\n",
        "print(\"Manual Leverage vs Statsmodels Leverage:\")\n",
        "print(np.allclose(leverage, statsmodels_leverage, atol=1e-02))\n",
        "\n",
        "print(\"Manual DFBETAS vs Statsmodels DFBETAS:\")\n",
        "print(np.allclose(dfbetas, statsmodels_dfbetas, atol=1e-02))\n",
        "\n",
        "print(\"Manual DFFITS (basic) vs Statsmodels DFFITS:\")\n",
        "print(np.allclose(dffits_basic, statsmodels_dffits, atol=1e-02))\n",
        "\n",
        "print(\"Manual DFFITS (PRESS-style) vs Statsmodels DFFITS:\")\n",
        "print(np.allclose(dffits_press, statsmodels_dffits, atol=1e-02))\n",
        "\n",
        "print(\"Manual Cook's Distance vs Statsmodels Cook's Distance:\")\n",
        "print(np.allclose(cooks_distances, statsmodels_cooks, atol=1e-02))\n",
        "\n",
        "comparison_snapshot = pd.DataFrame({\n",
        "    'Measure': ['Leverage', 'DFBETAS', 'DFFITS (PRESS)', \"Cook's Distance\"],\n",
        "    'Max |manual - statsmodels|': [\n",
        "        float(np.max(np.abs(leverage - statsmodels_leverage))),\n",
        "        float(np.max(np.abs(dfbetas - statsmodels_dfbetas))),\n",
        "        float(np.max(np.abs(dffits_press - statsmodels_dffits))),\n",
        "        float(np.max(np.abs(cooks_distances - statsmodels_cooks))),\n",
        "    ]\n",
        "})\n",
        "comparison_snapshot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdIRYM1skYGX"
      },
      "outputs": [],
      "source": [
        "\n",
        "def summarize_manual_influence_measures(model):\n",
        "    X = model.model.exog  # Design matrix\n",
        "    data = pd.DataFrame(model.model.data.frame)  # Original data as a DataFrame\n",
        "\n",
        "    # Compute influence measures using manual functions\n",
        "    leverage = manual_leverage(model)\n",
        "    dfbetas = manual_dfbetas(model)\n",
        "    dffits = manual_dffits_press(model)\n",
        "    cooks_distances = manual_cooks_distances(model)\n",
        "\n",
        "    # Number of observations and predictors\n",
        "    n, p = X.shape\n",
        "\n",
        "    # Rule-of-thumb thresholds\n",
        "    leverage_threshold = 2 * (p + 1) / n\n",
        "    cooks_distance_threshold = 4 / n\n",
        "    dffits_threshold = 2 * np.sqrt((p + 1) / n)\n",
        "    dfbetas_threshold = 2 / np.sqrt(n)\n",
        "\n",
        "    # Flag outliers based on thresholds\n",
        "    flagged = {\n",
        "        'High Leverage': leverage > leverage_threshold,\n",
        "        \"High Cook's Distance\": cooks_distances > cooks_distance_threshold,\n",
        "        'High DFFITS': np.abs(dffits) > dffits_threshold,\n",
        "    }\n",
        "\n",
        "    # Flag observations with high DFBETAS for any predictor\n",
        "    for j in range(p):\n",
        "        flagged[f'High DFBETAS (Predictor {j})'] = np.abs(dfbetas[:, j]) > dfbetas_threshold\n",
        "\n",
        "    # Create summary DataFrame\n",
        "    summary_df = pd.DataFrame({\n",
        "        'Leverage': leverage,\n",
        "        \"Cook's Distance\": cooks_distances,\n",
        "        'DFFITS': dffits,\n",
        "    })\n",
        "\n",
        "    # Add DFBETAS for each predictor\n",
        "    for j in range(p):\n",
        "        summary_df[f'DFBETAS (Predictor {j})'] = dfbetas[:, j]\n",
        "\n",
        "    # Add flags for rule-of-thumb violations\n",
        "    for key, flag in flagged.items():\n",
        "        summary_df[key] = flag\n",
        "\n",
        "    # Combine with original data\n",
        "    summary_with_data = pd.concat([data.reset_index(drop=True), summary_df], axis=1)\n",
        "\n",
        "    # Select flagged observations\n",
        "    flagged_columns = [col for col in summary_with_data.columns if col.startswith('High')]\n",
        "    flagged_observations = summary_with_data.loc[summary_with_data[flagged_columns].any(axis=1)]\n",
        "\n",
        "    return flagged_observations, summary_with_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IOZ1yHWNkaeb"
      },
      "outputs": [],
      "source": [
        "flagged_observations_manual, summary_with_data_manual = summarize_manual_influence_measures(model)\n",
        "flagged_observations_manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6g8NZRVgl3UM"
      },
      "outputs": [],
      "source": [
        "flagged_observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcjnabUPtwvG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Compare flagged observations from statsmodels vs manual calculations\n",
        "flag_cols_stats = [col for col in summary_with_data.columns if col.startswith('High')]\n",
        "flag_cols_manual = [col for col in summary_with_data_manual.columns if col.startswith('High')]\n",
        "\n",
        "stats_flagged_idx = set(summary_with_data.index[summary_with_data[flag_cols_stats].any(axis=1)])\n",
        "manual_flagged_idx = set(summary_with_data_manual.index[summary_with_data_manual[flag_cols_manual].any(axis=1)])\n",
        "\n",
        "print(f\"Statsmodels flagged: {len(stats_flagged_idx)} | Manual flagged: {len(manual_flagged_idx)}\")\n",
        "print(f\"Overlap: {len(stats_flagged_idx & manual_flagged_idx)}\")\n",
        "print(f\"Manual-only: {sorted(manual_flagged_idx - stats_flagged_idx)}\")\n",
        "print(f\"Stats-only: {sorted(stats_flagged_idx - manual_flagged_idx)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEFSC3nbx5Fc"
      },
      "outputs": [],
      "source": [
        "# Adding a good outlying point to predictors\n",
        "outlier = pd.DataFrame({'X1': [max(data0['X1']) + 25],\n",
        "                        'X2': [max(data0['X2']) + 35],\n",
        "                        'X3': [max(data0['X3']) + 25]})\n",
        "X_with_outlier = pd.concat([data0[['X1', 'X2', 'X3']], outlier], ignore_index=True)\n",
        "\n",
        "# Recalculating Y with the new outlying point\n",
        "# Create the design matrix for the model including the intercept\n",
        "X_design = sm.add_constant(X_with_outlier)\n",
        "# Calculate Y values including the outlier\n",
        "Y_with_outlier = np.dot(X_design, beta0).flatten() + np.append(e, np.random.normal(0, 4))\n",
        "\n",
        "# Simple Regression - only X2 as independent variable\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(X_with_outlier['X2'], Y_with_outlier)\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression with at least one influential point')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UuzUcHYyJuM"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Refit model including the artificially added outlier\n",
        "outlier_df = X_with_outlier.copy()\n",
        "outlier_df['Y'] = Y_with_outlier\n",
        "model_with_outlier = smf.ols('Y ~ X1 + X2 + X3', data=outlier_df).fit()\n",
        "\n",
        "# Compare coefficients before/after the outlier\n",
        "coef_compare = pd.DataFrame({\n",
        "    'Base model': model.params,\n",
        "    'With outlier': model_with_outlier.params,\n",
        "    'Delta': model_with_outlier.params - model.params\n",
        "})\n",
        "print('Coefficient shift after adding a leverage point:')\n",
        "print(coef_compare)\n",
        "\n",
        "# Influence plot to show the new point\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sm.graphics.influence_plot(model_with_outlier, criterion='cooks', size=60, ax=ax)\n",
        "ax.set_title(\"Influence plot with injected outlier\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE14OnGgsQ4m"
      },
      "source": [
        "##Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-ZUKnfOqVx4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Generate synthetic data\n",
        "np.random.seed(42)  # For reproducibility\n",
        "n = 100\n",
        "X1 = np.random.normal(10, 2, n)\n",
        "X2 = np.random.normal(20, 5, n)\n",
        "X3 = np.random.normal(30, 3, n)\n",
        "e = np.random.normal(0, 4, n)\n",
        "beta0 = [5, 2, -1, 3]  # Intercept and slopes for X1, X2, X3\n",
        "X_design = sm.add_constant(pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3}))  # Add intercept\n",
        "Y = np.dot(X_design, beta0) + e\n",
        "\n",
        "data0 = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'Y': Y})\n",
        "\n",
        "# Step 2: Function to add outliers or leverage points\n",
        "def add_outliers(data, n_outliers=1, leverage=False, extreme_y=False):\n",
        "    \"\"\"\n",
        "    Add outliers or high-leverage points to the data.\n",
        "\n",
        "    :param data: Original data as a DataFrame.\n",
        "    :param n_outliers: Number of outliers to add.\n",
        "    :param leverage: Whether to add high-leverage points (extreme predictors).\n",
        "    :param extreme_y: Whether to add extreme Y values.\n",
        "    :return: Updated DataFrame with added outliers.\n",
        "    \"\"\"\n",
        "    new_data = data.copy()\n",
        "    for _ in range(n_outliers):\n",
        "        if leverage:\n",
        "            # Add high-leverage points (extreme predictor values)\n",
        "            outlier = {\n",
        "                'X1': max(data['X1']) + np.random.uniform(20, 30),\n",
        "                'X2': max(data['X2']) + np.random.uniform(30, 40),\n",
        "                'X3': max(data['X3']) + np.random.uniform(20, 30),\n",
        "                'Y': np.random.uniform(min(data['Y']), max(data['Y']))\n",
        "            }\n",
        "        elif extreme_y:\n",
        "            # Add extreme Y values\n",
        "            outlier = {\n",
        "                'X1': np.random.uniform(min(data['X1']), max(data['X1'])),\n",
        "                'X2': np.random.uniform(min(data['X2']), max(data['X2'])),\n",
        "                'X3': np.random.uniform(min(data['X3']), max(data['X3'])),\n",
        "                'Y': max(data['Y']) + np.random.uniform(20, 40)\n",
        "            }\n",
        "        else:\n",
        "            # Add a general outlier (moderately extreme values in both X and Y)\n",
        "            outlier = {\n",
        "                'X1': max(data['X1']) + np.random.uniform(10, 20),\n",
        "                'X2': max(data['X2']) + np.random.uniform(15, 25),\n",
        "                'X3': max(data['X3']) + np.random.uniform(10, 20),\n",
        "                'Y': max(data['Y']) + np.random.uniform(10, 20)\n",
        "            }\n",
        "        new_data = pd.concat([new_data, pd.DataFrame([outlier])], ignore_index=True)\n",
        "    return new_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ise78u5aqnnu"
      },
      "outputs": [],
      "source": [
        "# Step 3: Add outliers or leverage points\n",
        "data_with_outliers = add_outliers(data0, n_outliers=3, leverage=True,extreme_y=True)\n",
        "\n",
        "# Step 4: Fit a regression model and calculate influence measures\n",
        "X_with_outliers = sm.add_constant(data_with_outliers[['X1', 'X2', 'X3']])\n",
        "model = sm.OLS(data_with_outliers['Y'], X_with_outliers).fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnAj3-KCsxq1"
      },
      "outputs": [],
      "source": [
        "# Step 5: Visualize scatter plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(data_with_outliers['X2'], data_with_outliers['Y'], label=\"Data Points\", alpha=0.7)\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Simple Regression with Added Outliers')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 6: Plot regression diagnostics\n",
        "fig = model.get_influence().summary_frame().plot(kind='scatter', x='hat_diag', y='student_resid', alpha=0.7)\n",
        "plt.title('Regression Diagnostics: Leverage vs Studentized Residuals')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Highlight flagged observations\n",
        "influence = model.get_influence()\n",
        "summary_frame = influence.summary_frame()\n",
        "summary_frame['index'] = range(len(summary_frame))\n",
        "flagged_obs = summary_frame[\n",
        "    (summary_frame['cooks_d'] > 4 / n) | (summary_frame['hat_diag'] > 2 * (X_with_outliers.shape[1] / n))\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.scatter(data_with_outliers['X2'], data_with_outliers['Y'], label=\"Data Points\", alpha=0.7)\n",
        "plt.scatter(\n",
        "    data_with_outliers.iloc[flagged_obs['index']]['X2'],\n",
        "    data_with_outliers.iloc[flagged_obs['index']]['Y'],\n",
        "    color='red',\n",
        "    label='Flagged Observations',\n",
        "    s=100\n",
        ")\n",
        "plt.xlabel('X2')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Flagged Observations Highlighted')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1mzBCcKqWSS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Leverage vs externally studentized residuals with Cook's distance shading\n",
        "influence = model.get_influence()\n",
        "leverage_vals = influence.hat_matrix_diag\n",
        "student_resid_ext = influence.resid_studentized_external\n",
        "cooks = influence.cooks_distance[0]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "scatter = plt.scatter(leverage_vals, student_resid_ext, c=cooks, cmap='viridis', alpha=0.8)\n",
        "plt.colorbar(scatter, label=\"Cook's Distance\")\n",
        "\n",
        "p = len(model.params)\n",
        "n = int(model.nobs)\n",
        "leverage_threshold = 2 * p / n\n",
        "plt.axvline(leverage_threshold, color='red', linestyle='--', label='High leverage rule')\n",
        "plt.axhline(0, color='black', linewidth=0.8)\n",
        "plt.axhline(2, color='orange', linestyle='--', linewidth=0.8)\n",
        "plt.axhline(-2, color='orange', linestyle='--', linewidth=0.8)\n",
        "plt.xlabel('Leverage (h_ii)')\n",
        "plt.ylabel('Externally studentized residuals')\n",
        "plt.title('Leverage vs Studentized Residuals (Cook coloring)')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjccDAFf8W4Q"
      },
      "source": [
        "\n",
        "### HW: Regression Diagnostics on mpg with Injected Influence\n",
        "\n",
        "Practice the diagnostics techniques from **Ex08**: residual analysis, leverage, Cook's distance, DFFITS/DFBETAS, and model refitting after removing influential observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3wGvvtQ8W4Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load and clean the dataset, then create a 90/10 train/holdout split\n",
        "base_data = sns.load_dataset('mpg').dropna()\n",
        "base_data = base_data.drop(columns=['name'])  # drop redundant string column\n",
        "base_data = base_data.reset_index(drop=True)\n",
        "\n",
        "train_frac = 0.9\n",
        "train_idx = base_data.sample(frac=train_frac, random_state=123).index\n",
        "train_base = base_data.loc[train_idx].reset_index(drop=True)\n",
        "holdout = base_data.drop(train_idx).reset_index(drop=True)\n",
        "\n",
        "print(f\"Base shape: {base_data.shape} | Train: {train_base.shape} | Holdout: {holdout.shape}\")\n",
        "train_base.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohcy5Hd18W4Q"
      },
      "outputs": [],
      "source": [
        "# Inject 10 influential points into the training set only\n",
        "rng = np.random.default_rng(42)\n",
        "train_mod = train_base.copy()\n",
        "train_mod['flag'] = 'train_original'\n",
        "\n",
        "# 5 leverage points: extreme predictors, modest mpg to avoid immediate y-outlier status\n",
        "leverage_rows = train_base.sample(5, random_state=1).copy()\n",
        "leverage_rows['horsepower'] = leverage_rows['horsepower'] * 2.2\n",
        "leverage_rows['weight'] = leverage_rows['weight'] * 1.9\n",
        "leverage_rows['acceleration'] = leverage_rows['acceleration'] * 0.6\n",
        "leverage_rows['mpg'] = leverage_rows['mpg'] * rng.uniform(0.9, 1.1, size=5)\n",
        "leverage_rows['flag'] = 'train_leverage'\n",
        "\n",
        "# 5 outliers: normal predictors, extreme mpg values\n",
        "outlier_rows = train_base.sample(5, random_state=7).copy()\n",
        "outlier_rows['mpg'] = outlier_rows['mpg'].mean() + rng.choice([-1, 1], size=5) * rng.uniform(20, 35, size=5)\n",
        "outlier_rows['flag'] = 'train_outlier'\n",
        "\n",
        "train_mod = pd.concat([train_mod, leverage_rows, outlier_rows], ignore_index=True)\n",
        "train_mod = train_mod.reset_index(drop=True)\n",
        "\n",
        "holdout_mod = holdout.copy()\n",
        "holdout_mod['flag'] = 'holdout'\n",
        "\n",
        "print('Training flags:')\n",
        "print(train_mod['flag'].value_counts())\n",
        "train_mod.tail(12)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8lRMwCd8W4Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inject 10 influential points: 5 high-leverage predictors, 5 extreme-response outliers\n",
        "rng = np.random.default_rng(42)\n",
        "modified = base_data.copy()\n",
        "\n",
        "# 5 leverage points: extreme predictors, reasonable mpg to avoid immediate flag as y-outliers\n",
        "leverage_rows = base_data.sample(5, random_state=1).copy()\n",
        "leverage_rows['horsepower'] = leverage_rows['horsepower'] * 2.2\n",
        "leverage_rows['weight'] = leverage_rows['weight'] * 1.9\n",
        "leverage_rows['acceleration'] = leverage_rows['acceleration'] * 0.6\n",
        "leverage_rows['mpg'] = leverage_rows['mpg'] * rng.uniform(0.9, 1.1, size=5)\n",
        "\n",
        "# 5 outliers: normal predictors, extreme mpg values\n",
        "outlier_rows = base_data.sample(5, random_state=7).copy()\n",
        "outlier_rows['mpg'] = outlier_rows['mpg'].mean() + rng.choice([-1, 1], size=5) * rng.uniform(20, 35, size=5)\n",
        "\n",
        "# Tag the injected rows for later identification\n",
        "leverage_rows['flag'] = 'leverage'\n",
        "outlier_rows['flag'] = 'outlier'\n",
        "\n",
        "modified['flag'] = 'original'\n",
        "modified = pd.concat([modified, leverage_rows, outlier_rows], ignore_index=True)\n",
        "modified = modified.reset_index(drop=True)\n",
        "modified.tail(12)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-UyiNOw8W4Q"
      },
      "source": [
        "\n",
        "## Homework\n",
        "Use the modified dataset mpg_influential\n",
        "\n",
        "1. **Load & EDA**: Load mpg_influential.csv, inspect missing values, summarize stats, and visualize pairwise relationships (e.g., pairplot/regplots for mpg vs key predictors).\n",
        "\n",
        "2. **Baseline model**: Fit an OLS model predicting mpg from a sensible subset. Show the summary.  Score the baseline model on the holdout set (RMSE/MAE/R2). Keep this as your reference before cleaning.\n",
        "\n",
        "3. **Residual visuals**: Plot residuals vs fitted, scale-location, and QQ plot. Briefly note any patterns.\n",
        "\n",
        "4. **Partial plots**: Create component-plus-residual (CCPR) and added-variable plots for key predictors. Comment on linearity and influential points.\n",
        "\n",
        "5. **Influence table**: Compute leverage, Cook's distance, DFFITS, and DFBETAS. Add rule-of-thumb flags (as in Ex08). List the top 15 influential points by Cook's distance.\n",
        "\n",
        "6. **Identify injected points**: Cross-check which flagged observations correspond to flag in the dataset. How many of the injected leverage/outlier rows are detected?\n",
        "\n",
        "7. **Visualization of influence**: Make a leverage vs studentized residual plot colored by Cook's distance, marking thresholds. Highlight flagged points.\n",
        "\n",
        "8. **Refit without influential points**: Remove observations exceeding your chosen thresholds (you can justify them). Refit the model and compare coefficients/RMSE/R2 to the baseline.\n",
        "\n",
        "9. **Prediction impact**: For a holdout test df, compare predicted mpg before vs after cleaning. Discuss stability.\n",
        "\n",
        "10. **Reflection**: Summarize which diagnostics were most informative and how leverage vs outlier behavior differed in this dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "TezRLBirS2OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGfOLzqV8W4Q"
      },
      "outputs": [],
      "source": [
        "# Q1: EDA\n",
        "mpg_influent_df = modified.copy()\n",
        "\n",
        "# 1) Missing values\n",
        "print('Missing values:', mpg_influent_df.isna().sum().to_dict(), '\\n')\n",
        "\n",
        "# 2) Summary stats\n",
        "print('Summary statistics')\n",
        "mpg_influent_df.describe()\n",
        "\n",
        "# 3) Pairwise relationships\n",
        "# 3a) Correlation matrix\n",
        "numerical_predictors = ['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model_year']\n",
        "categorical_predictors = ['flag', 'origin']\n",
        "corr = mpg_influent_df[numerical_predictors].corr()\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
        "plt.title('Correlation heatmap (predictors vs Y)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3b) Pairwise plot\n",
        "\n",
        "sns.pairplot(\n",
        "    mpg_influent_df,\n",
        "    x_vars=numerical_predictors,\n",
        "    y_vars=['mpg'],\n",
        "    kind='scatter',\n",
        "    plot_kws={\"alpha\": 0.6}\n",
        ")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mpg_influent_df"
      ],
      "metadata": {
        "id": "F2Irg2ceVESQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toZtkBSJ8W4R"
      },
      "outputs": [],
      "source": [
        "# Q2: Baseline OLS\n",
        "# 1) Fitting model\n",
        "query = 'mpg ~ ' + ' + '.join(numerical_predictors+categorical_predictors)\n",
        "print(query)\n",
        "model = smf.ols(query, data=mpg_influent_df).fit()\n",
        "\n",
        "print('Model summary:')\n",
        "print(model.summary())\n",
        "\n",
        "# 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j21g_nzd8W4R"
      },
      "outputs": [],
      "source": [
        "# Q3: Residual visuals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtE-endM8W4R"
      },
      "outputs": [],
      "source": [
        "# Q4: CCPR / Added-variable plots\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPCtjt1i8W4R"
      },
      "outputs": [],
      "source": [
        "# Q5: Influence table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b92rRf_8W4R"
      },
      "outputs": [],
      "source": [
        "# Q6: Identify injected points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O4_1lSwH8W4R"
      },
      "outputs": [],
      "source": [
        "# Q7: Influence visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go7IZrOf8W4R"
      },
      "outputs": [],
      "source": [
        "# Q8: Refit without influential points\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISFMJbE-8W4R"
      },
      "outputs": [],
      "source": [
        "# Q9: Prediction impact\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1dV0Zsp8W4S"
      },
      "source": [
        "Summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHnTIuxB8W4S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}